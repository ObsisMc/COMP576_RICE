{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02dfde51",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-09T02:50:02.368498Z",
     "iopub.status.busy": "2023-11-09T02:50:02.368158Z",
     "iopub.status.idle": "2023-11-09T02:50:06.792856Z",
     "shell.execute_reply": "2023-11-09T02:50:06.791987Z"
    },
    "papermill": {
     "duration": 4.43297,
     "end_time": "2023-11-09T02:50:06.795314",
     "exception": false,
     "start_time": "2023-11-09T02:50:02.362344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "# import psutil\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "is_offline = False\n",
    "is_train = True\n",
    "is_infer = False\n",
    "max_lookback = np.nan\n",
    "split_day = 435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a8c65b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T02:50:06.807000Z",
     "iopub.status.busy": "2023-11-09T02:50:06.806681Z",
     "iopub.status.idle": "2023-11-09T02:50:23.725160Z",
     "shell.execute_reply": "2023-11-09T02:50:23.724267Z"
    },
    "papermill": {
     "duration": 16.926188,
     "end_time": "2023-11-09T02:50:23.727256",
     "exception": false,
     "start_time": "2023-11-09T02:50:06.801068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237892, 17)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8363e73a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T02:50:23.737634Z",
     "iopub.status.busy": "2023-11-09T02:50:23.737362Z",
     "iopub.status.idle": "2023-11-09T02:50:24.424701Z",
     "shell.execute_reply": "2023-11-09T02:50:24.423837Z"
    },
    "papermill": {
     "duration": 0.695126,
     "end_time": "2023-11-09T02:50:24.426821",
     "exception": false,
     "start_time": "2023-11-09T02:50:23.731695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "# generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    # V1\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    # V2\n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "    # V3\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n",
    "                'wap', 'near_price', 'far_price','market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# generate time & stock features\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5\n",
    "    df[\"dom\"] = df[\"date_id\"] % 20\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "    return df\n",
    "\n",
    "# 添加基础排名因子\n",
    "def dfrank(newdf): \n",
    "    columns=[column for column in newdf.columns if (\n",
    "            ('target' not in column) # 这个是预测目标，不能排名\n",
    "        and ('date_id' not in column) # 这个在回测时不显示，不能使用\n",
    "        and ('time_id' not in column) # 这个在回测时不显示，不能使用\n",
    "        and ('row_id' not in column)\n",
    "        and ('stock_id' not in column)\n",
    "        and ('seconds_in_bucket' not in column)\n",
    "        )]\n",
    "    for column in columns:\n",
    "        # 从小到大排名【测试下双排名有效果是因为加上了na_option='bottom'的处理机制还是因为实现的双排名方案】\n",
    "        newdf=pd.concat([newdf,(newdf[str(column)].rank(method=\"max\", ascending=False,na_option='bottom')/len(newdf)).rename(f\"{str(column)}_rank\")], axis=1)\n",
    "        # 从大到小排名\n",
    "        newdf=pd.concat([newdf,(newdf[str(column)].rank(method=\"max\", ascending=True,na_option='bottom')/len(newdf)).rename(f\"{str(column)}_rerank\")], axis=1) # 从大到小排序\n",
    "    return newdf\n",
    "\n",
    "# generate all features\n",
    "def generate_all_features(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # 计算基础排名因子\n",
    "    df = df.groupby(['date_id','seconds_in_bucket']).apply(dfrank) # 计算排名因子\n",
    "    df = df.reset_index(drop=True) # 重置索引并且不使其生成新的列【提交之前尽量对df的格式进行还原,需要注意这俩索引的值在分组之后是保留了的，如果不去掉容易造成资源竞争】\n",
    "    df = reduce_mem_usage(df)\n",
    "#     print(\"基础排名因子\")\n",
    "#     mem = psutil.virtual_memory()\n",
    "#     zj = float(mem.total) / 1024 / 1024 / 1024\n",
    "#     ysy = float(mem.used) / 1024 / 1024 / 1024\n",
    "#     kx = float(mem.free) / 1024 / 1024 / 1024\n",
    "#     print('系统总计内存:%d.3GB' % zj)\n",
    "#     print('系统已经使用内存:%d.3GB' % ysy)\n",
    "#     print('系统空闲内存:%d.3GB' % kx)\n",
    "    gc.collect()\n",
    "    df = imbalance_features(df)\n",
    "    df = reduce_mem_usage(df)\n",
    "#     print(\"不平衡特征\")\n",
    "    gc.collect()\n",
    "    df = other_features(df)\n",
    "    df = reduce_mem_usage(df)\n",
    "#     print(\"其余特征\")\n",
    "    gc.collect()\n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    return df[feature_name]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}\n",
    "\n",
    "if is_offline:\n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556d4e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T02:50:24.436913Z",
     "iopub.status.busy": "2023-11-09T02:50:24.436644Z",
     "iopub.status.idle": "2023-11-09T03:01:47.010344Z",
     "shell.execute_reply": "2023-11-09T03:01:47.009280Z"
    },
    "papermill": {
     "duration": 682.581203,
     "end_time": "2023-11-09T03:01:47.012646",
     "exception": false,
     "start_time": "2023-11-09T02:50:24.431443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "        \n",
    "df_train_feats = reduce_mem_usage(df_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d5fd0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T03:01:47.023438Z",
     "iopub.status.busy": "2023-11-09T03:01:47.023128Z",
     "iopub.status.idle": "2023-11-09T03:01:47.027546Z",
     "shell.execute_reply": "2023-11-09T03:01:47.026622Z"
    },
    "papermill": {
     "duration": 0.011872,
     "end_time": "2023-11-09T03:01:47.029534",
     "exception": false,
     "start_time": "2023-11-09T03:01:47.017662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_train_feats.reset_index(drop = True).to_feather(\"/kaggle/working/df_train_Nov6.fea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9fcc6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T03:01:47.040417Z",
     "iopub.status.busy": "2023-11-09T03:01:47.039031Z",
     "iopub.status.idle": "2023-11-09T03:01:47.045001Z",
     "shell.execute_reply": "2023-11-09T03:01:47.044048Z"
    },
    "papermill": {
     "duration": 0.01296,
     "end_time": "2023-11-09T03:01:47.046825",
     "exception": false,
     "start_time": "2023-11-09T03:01:47.033865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_feats_length:  5237892\n"
     ]
    }
   ],
   "source": [
    "print(\"df_train_feats_length: \", len(df_train_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c288d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T03:01:47.056740Z",
     "iopub.status.busy": "2023-11-09T03:01:47.056479Z",
     "iopub.status.idle": "2023-11-09T03:01:48.029951Z",
     "shell.execute_reply": "2023-11-09T03:01:48.029063Z"
    },
    "papermill": {
     "duration": 0.980751,
     "end_time": "2023-11-09T03:01:48.032058",
     "exception": false,
     "start_time": "2023-11-09T03:01:47.051307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>...</th>\n",
       "      <th>dow</th>\n",
       "      <th>dom</th>\n",
       "      <th>seconds</th>\n",
       "      <th>minute</th>\n",
       "      <th>global_median_size</th>\n",
       "      <th>global_std_size</th>\n",
       "      <th>global_ptp_size</th>\n",
       "      <th>global_median_price</th>\n",
       "      <th>global_std_price</th>\n",
       "      <th>global_ptp_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.180603e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380277.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>60651.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42739.160156</td>\n",
       "      <td>132986.921875</td>\n",
       "      <td>5.898990e+06</td>\n",
       "      <td>1.999695</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>0.017414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666039e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>3233.040039</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25548.500000</td>\n",
       "      <td>66444.906250</td>\n",
       "      <td>6.938986e+05</td>\n",
       "      <td>1.999827</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.029370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.028799e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>37956.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26228.099609</td>\n",
       "      <td>75674.656250</td>\n",
       "      <td>1.069838e+06</td>\n",
       "      <td>2.000200</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.051622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191768e+07</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389746.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2324.899902</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41667.000000</td>\n",
       "      <td>93875.773438</td>\n",
       "      <td>1.928848e+06</td>\n",
       "      <td>1.999980</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.018551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4.475500e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>16485.539062</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34014.578125</td>\n",
       "      <td>80670.273438</td>\n",
       "      <td>1.604066e+06</td>\n",
       "      <td>1.999816</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.017379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237887</th>\n",
       "      <td>195</td>\n",
       "      <td>540</td>\n",
       "      <td>2.440723e+06</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>28280362.00</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>32257.039062</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>51941.550781</td>\n",
       "      <td>98218.031250</td>\n",
       "      <td>2.761659e+06</td>\n",
       "      <td>1.999930</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.014076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237888</th>\n",
       "      <td>196</td>\n",
       "      <td>540</td>\n",
       "      <td>3.495105e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>9187699.00</td>\n",
       "      <td>1.000129</td>\n",
       "      <td>1.000386</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>205108.406250</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42476.949219</td>\n",
       "      <td>78070.062500</td>\n",
       "      <td>4.596574e+05</td>\n",
       "      <td>2.000042</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.017398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237889</th>\n",
       "      <td>197</td>\n",
       "      <td>540</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>12725436.00</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>16790.660156</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>30070.039062</td>\n",
       "      <td>71964.171875</td>\n",
       "      <td>1.575294e+06</td>\n",
       "      <td>1.999984</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.020387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237890</th>\n",
       "      <td>198</td>\n",
       "      <td>540</td>\n",
       "      <td>1.000899e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>94773272.00</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>125631.718750</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>304739.250000</td>\n",
       "      <td>354682.781250</td>\n",
       "      <td>2.159163e+06</td>\n",
       "      <td>1.999917</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.015738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237891</th>\n",
       "      <td>199</td>\n",
       "      <td>540</td>\n",
       "      <td>1.884286e+06</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>24073678.00</td>\n",
       "      <td>1.000859</td>\n",
       "      <td>1.001494</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>250081.437500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>114127.101562</td>\n",
       "      <td>194210.062500</td>\n",
       "      <td>4.564502e+06</td>\n",
       "      <td>2.000128</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>0.022793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5237892 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stock_id  seconds_in_bucket  imbalance_size  imbalance_buy_sell_flag  \\\n",
       "0               0                  0    3.180603e+06                        1   \n",
       "1               1                  0    1.666039e+05                       -1   \n",
       "2               2                  0    3.028799e+05                       -1   \n",
       "3               3                  0    1.191768e+07                       -1   \n",
       "4               4                  0    4.475500e+05                       -1   \n",
       "...           ...                ...             ...                      ...   \n",
       "5237887       195                540    2.440723e+06                       -1   \n",
       "5237888       196                540    3.495105e+05                       -1   \n",
       "5237889       197                540    0.000000e+00                        0   \n",
       "5237890       198                540    1.000899e+06                        1   \n",
       "5237891       199                540    1.884286e+06                       -1   \n",
       "\n",
       "         reference_price  matched_size  far_price  near_price  bid_price  \\\n",
       "0               0.999812   13380277.00        NaN         NaN   0.999812   \n",
       "1               0.999896    1642214.25        NaN         NaN   0.999896   \n",
       "2               0.999561    1819368.00        NaN         NaN   0.999403   \n",
       "3               1.000171   18389746.00        NaN         NaN   0.999999   \n",
       "4               0.999532   17860614.00        NaN         NaN   0.999394   \n",
       "...                  ...           ...        ...         ...        ...   \n",
       "5237887         1.000317   28280362.00   0.999734    0.999734   1.000317   \n",
       "5237888         1.000643    9187699.00   1.000129    1.000386   1.000643   \n",
       "5237889         0.995789   12725436.00   0.995789    0.995789   0.995789   \n",
       "5237890         0.999210   94773272.00   0.999210    0.999210   0.998970   \n",
       "5237891         1.002129   24073678.00   1.000859    1.001494   1.002129   \n",
       "\n",
       "              bid_size  ...  dow  dom  seconds  minute  global_median_size  \\\n",
       "0         60651.500000  ...    0    0        0       0        42739.160156   \n",
       "1          3233.040039  ...    0    0        0       0        25548.500000   \n",
       "2         37956.000000  ...    0    0        0       0        26228.099609   \n",
       "3          2324.899902  ...    0    0        0       0        41667.000000   \n",
       "4         16485.539062  ...    0    0        0       0        34014.578125   \n",
       "...                ...  ...  ...  ...      ...     ...                 ...   \n",
       "5237887   32257.039062  ...    0    0        0       9        51941.550781   \n",
       "5237888  205108.406250  ...    0    0        0       9        42476.949219   \n",
       "5237889   16790.660156  ...    0    0        0       9        30070.039062   \n",
       "5237890  125631.718750  ...    0    0        0       9       304739.250000   \n",
       "5237891  250081.437500  ...    0    0        0       9       114127.101562   \n",
       "\n",
       "         global_std_size  global_ptp_size  global_median_price  \\\n",
       "0          132986.921875     5.898990e+06             1.999695   \n",
       "1           66444.906250     6.938986e+05             1.999827   \n",
       "2           75674.656250     1.069838e+06             2.000200   \n",
       "3           93875.773438     1.928848e+06             1.999980   \n",
       "4           80670.273438     1.604066e+06             1.999816   \n",
       "...                  ...              ...                  ...   \n",
       "5237887     98218.031250     2.761659e+06             1.999930   \n",
       "5237888     78070.062500     4.596574e+05             2.000042   \n",
       "5237889     71964.171875     1.575294e+06             1.999984   \n",
       "5237890    354682.781250     2.159163e+06             1.999917   \n",
       "5237891    194210.062500     4.564502e+06             2.000128   \n",
       "\n",
       "         global_std_price  global_ptp_price  \n",
       "0                0.003353          0.017414  \n",
       "1                0.005588          0.029370  \n",
       "2                0.005333          0.051622  \n",
       "3                0.002903          0.018551  \n",
       "4                0.003717          0.017379  \n",
       "...                   ...               ...  \n",
       "5237887          0.003051          0.014076  \n",
       "5237888          0.003416          0.017398  \n",
       "5237889          0.004696          0.020387  \n",
       "5237890          0.003146          0.015738  \n",
       "5237891          0.004325          0.022793  \n",
       "\n",
       "[5237892 rows x 184 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6920c78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T03:01:48.044147Z",
     "iopub.status.busy": "2023-11-09T03:01:48.043788Z",
     "iopub.status.idle": "2023-11-09T03:01:48.051042Z",
     "shell.execute_reply": "2023-11-09T03:01:48.050343Z"
    },
    "papermill": {
     "duration": 0.01548,
     "end_time": "2023-11-09T03:01:48.052968",
     "exception": false,
     "start_time": "2023-11-09T03:01:48.037488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if is_train:\n",
    "#     feature_name = list(df_train_feats.columns)\n",
    "#     # 3.399\n",
    "# #     lgb_params = {\n",
    "# #         \"objective\" : \"mae\",\n",
    "# #         \"n_estimators\" : 6000,\n",
    "# #         \"num_leaves\" : 512,\n",
    "# #         \"subsample\" : 0.65,\n",
    "# #         \"colsample_bytree\" : 0.65,\n",
    "# #         \"learning_rate\" : 0.007,\n",
    "# #         \"n_jobs\" : 4,\n",
    "# #         \"device\" : \"gpu\",\n",
    "# #         \"verbosity\": -1,\n",
    "# #         \"importance_type\" : \"gain\",\n",
    "# #         \"max_depth\": 12,  # Maximum depth of the tree\n",
    "# #         \"min_child_samples\": 15,  # Minimum number of data points in a leaf\n",
    "# #         \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "# #         \"reg_lambda\": 0.3,  # L2 regularization term\n",
    "# #         \"min_split_gain\": 0.2,  # Minimum loss reduction required for further partitioning\n",
    "# #         \"min_child_weight\": 0.001,  # Minimum sum of instance weight (hessian) in a leaf\n",
    "# #         \"bagging_fraction\": 0.9,  # Fraction of data to be used for training each tree\n",
    "# #         \"bagging_freq\": 5,  # Frequency for bagging\n",
    "# #         \"feature_fraction\": 0.9,  # Fraction of features to be used for training each tree\n",
    "# #         \"num_threads\": 4,  # Number of threads for LightGBM to use\n",
    "# #     }\n",
    "#     lgb_params = {\n",
    "#         \"objective\": \"mae\",\n",
    "#         \"n_estimators\": 6000,\n",
    "#         \"num_leaves\": 384,  # 比较确定会更好\n",
    "#         \"subsample\": 0.6,   # 比较确定会更好\n",
    "#         \"colsample_bytree\": 0.8,\n",
    "#         \"learning_rate\": 0.1, # 0.00871 学习率影响不大\n",
    "#         'max_depth': 12,\n",
    "#         \"n_jobs\": 4,\n",
    "#         \"device\": \"gpu\",\n",
    "#         \"verbosity\": -1,\n",
    "#         \"importance_type\": \"gain\",\n",
    "#     }\n",
    "    \n",
    "\n",
    "#     print(f\"Feature length = {len(feature_name)}\")\n",
    "#     offline_split = df_train['date_id']>(split_day - 45)\n",
    "# #     df_offline_train = df_train_feats[~offline_split]\n",
    "# #     df_offline_valid = df_train_feats[offline_split]\n",
    "# #     df_offline_train_target = df_train['target'][~offline_split]\n",
    "# #     df_offline_valid_target = df_train['target'][offline_split]\n",
    "\n",
    "#     print(\"Valid Model Trainning.\")\n",
    "#     lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "#     lgb_model.fit(\n",
    "#         df_train_feats[~offline_split][feature_name],\n",
    "#         df_train['target'][~offline_split],\n",
    "#         eval_set=[(df_train_feats[offline_split][feature_name], df_train['target'][offline_split])],\n",
    "#         callbacks=[\n",
    "#             lgb.callback.early_stopping(stopping_rounds=100),\n",
    "#             lgb.callback.log_evaluation(period=100),\n",
    "#         ],\n",
    "#     )\n",
    "\n",
    "# #     del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target\n",
    "#     gc.collect()\n",
    "#     # infer\n",
    "#     df_train_target = df_train[\"target\"]\n",
    "#     print(\"Infer Model Trainning.\")\n",
    "#     infer_params = lgb_params.copy()\n",
    "#     infer_params[\"n_estimators\"] = int(1.2 * lgb_model.best_iteration_)\n",
    "#     infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n",
    "#     infer_lgb_model.fit(df_train_feats[feature_name], df_train_target)\n",
    "#     if is_offline:   \n",
    "#         # offline predictions\n",
    "#         df_valid_target = df_valid[\"target\"]\n",
    "#         offline_predictions = infer_lgb_model.predict(df_valid_feats[feature_name])\n",
    "#         offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n",
    "#         print(f\"Offline Score {np.round(offline_score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69163770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T03:01:48.064213Z",
     "iopub.status.busy": "2023-11-09T03:01:48.063957Z",
     "iopub.status.idle": "2023-11-09T06:27:58.567589Z",
     "shell.execute_reply": "2023-11-09T06:27:58.566570Z"
    },
    "papermill": {
     "duration": 12370.538557,
     "end_time": "2023-11-09T06:27:58.596476",
     "exception": false,
     "start_time": "2023-11-09T03:01:48.057919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length = 184\n",
      "Fold 1 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 7.09465\n",
      "[200]\tvalid_0's l1: 7.04375\n",
      "[300]\tvalid_0's l1: 7.01561\n",
      "[400]\tvalid_0's l1: 6.99261\n",
      "[500]\tvalid_0's l1: 6.97344\n",
      "[600]\tvalid_0's l1: 6.95565\n",
      "[700]\tvalid_0's l1: 6.93968\n",
      "[800]\tvalid_0's l1: 6.92541\n",
      "[900]\tvalid_0's l1: 6.91291\n",
      "[1000]\tvalid_0's l1: 6.9008\n",
      "[1100]\tvalid_0's l1: 6.88944\n",
      "[1200]\tvalid_0's l1: 6.87882\n",
      "[1300]\tvalid_0's l1: 6.86879\n",
      "[1400]\tvalid_0's l1: 6.85951\n",
      "[1500]\tvalid_0's l1: 6.85118\n",
      "[1600]\tvalid_0's l1: 6.84226\n",
      "[1700]\tvalid_0's l1: 6.83432\n",
      "[1800]\tvalid_0's l1: 6.82669\n",
      "[1900]\tvalid_0's l1: 6.81843\n",
      "[2000]\tvalid_0's l1: 6.81079\n",
      "[2100]\tvalid_0's l1: 6.80359\n",
      "[2200]\tvalid_0's l1: 6.79636\n",
      "[2300]\tvalid_0's l1: 6.78868\n",
      "[2400]\tvalid_0's l1: 6.78084\n",
      "[2500]\tvalid_0's l1: 6.77416\n",
      "[2600]\tvalid_0's l1: 6.76729\n",
      "[2700]\tvalid_0's l1: 6.7607\n",
      "[2800]\tvalid_0's l1: 6.7534\n",
      "[2900]\tvalid_0's l1: 6.74603\n",
      "[3000]\tvalid_0's l1: 6.74027\n",
      "[3100]\tvalid_0's l1: 6.73303\n",
      "[3200]\tvalid_0's l1: 6.72582\n",
      "[3300]\tvalid_0's l1: 6.71995\n",
      "[3400]\tvalid_0's l1: 6.71471\n",
      "[3500]\tvalid_0's l1: 6.70891\n",
      "[3600]\tvalid_0's l1: 6.70284\n",
      "[3700]\tvalid_0's l1: 6.6968\n",
      "[3800]\tvalid_0's l1: 6.69025\n",
      "[3900]\tvalid_0's l1: 6.68521\n",
      "[4000]\tvalid_0's l1: 6.67947\n",
      "[4100]\tvalid_0's l1: 6.67416\n",
      "[4200]\tvalid_0's l1: 6.66813\n",
      "[4300]\tvalid_0's l1: 6.6623\n",
      "[4400]\tvalid_0's l1: 6.65692\n",
      "[4500]\tvalid_0's l1: 6.65199\n",
      "[4600]\tvalid_0's l1: 6.64723\n",
      "[4700]\tvalid_0's l1: 6.64218\n",
      "[4800]\tvalid_0's l1: 6.63727\n",
      "[4900]\tvalid_0's l1: 6.63232\n",
      "[5000]\tvalid_0's l1: 6.62624\n",
      "[5100]\tvalid_0's l1: 6.62079\n",
      "[5200]\tvalid_0's l1: 6.61539\n",
      "[5300]\tvalid_0's l1: 6.61008\n",
      "[5400]\tvalid_0's l1: 6.60453\n",
      "[5500]\tvalid_0's l1: 6.59897\n",
      "[5600]\tvalid_0's l1: 6.59332\n",
      "[5700]\tvalid_0's l1: 6.58816\n",
      "[5800]\tvalid_0's l1: 6.58337\n",
      "[5900]\tvalid_0's l1: 6.57887\n",
      "[6000]\tvalid_0's l1: 6.57475\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\tvalid_0's l1: 6.57475\n",
      "Model for fold 1 saved to modelitos_para_despues/doblez_1.txt\n",
      "Fold 1 MAE: 6.574749414892834\n",
      "Fold 2 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.41791\n",
      "[200]\tvalid_0's l1: 6.38139\n",
      "[300]\tvalid_0's l1: 6.35843\n",
      "[400]\tvalid_0's l1: 6.33823\n",
      "[500]\tvalid_0's l1: 6.32081\n",
      "[600]\tvalid_0's l1: 6.30537\n",
      "[700]\tvalid_0's l1: 6.29073\n",
      "[800]\tvalid_0's l1: 6.27686\n",
      "[900]\tvalid_0's l1: 6.26439\n",
      "[1000]\tvalid_0's l1: 6.25272\n",
      "[1100]\tvalid_0's l1: 6.24204\n",
      "[1200]\tvalid_0's l1: 6.23214\n",
      "[1300]\tvalid_0's l1: 6.22278\n",
      "[1400]\tvalid_0's l1: 6.21386\n",
      "[1500]\tvalid_0's l1: 6.20521\n",
      "[1600]\tvalid_0's l1: 6.19733\n",
      "[1700]\tvalid_0's l1: 6.18941\n",
      "[1800]\tvalid_0's l1: 6.18118\n",
      "[1900]\tvalid_0's l1: 6.17328\n",
      "[2000]\tvalid_0's l1: 6.16492\n",
      "[2100]\tvalid_0's l1: 6.15787\n",
      "[2200]\tvalid_0's l1: 6.14963\n",
      "[2300]\tvalid_0's l1: 6.14121\n",
      "[2400]\tvalid_0's l1: 6.13344\n",
      "[2500]\tvalid_0's l1: 6.12626\n",
      "[2600]\tvalid_0's l1: 6.11968\n",
      "[2700]\tvalid_0's l1: 6.11219\n",
      "[2800]\tvalid_0's l1: 6.10605\n",
      "[2900]\tvalid_0's l1: 6.09933\n",
      "[3000]\tvalid_0's l1: 6.09277\n",
      "[3100]\tvalid_0's l1: 6.08726\n",
      "[3200]\tvalid_0's l1: 6.08047\n",
      "[3300]\tvalid_0's l1: 6.07441\n",
      "[3400]\tvalid_0's l1: 6.06839\n",
      "[3500]\tvalid_0's l1: 6.06179\n",
      "[3600]\tvalid_0's l1: 6.05483\n",
      "[3700]\tvalid_0's l1: 6.04832\n",
      "[3800]\tvalid_0's l1: 6.04204\n",
      "[3900]\tvalid_0's l1: 6.03684\n",
      "[4000]\tvalid_0's l1: 6.03156\n",
      "[4100]\tvalid_0's l1: 6.02613\n",
      "[4200]\tvalid_0's l1: 6.02101\n",
      "[4300]\tvalid_0's l1: 6.01659\n",
      "[4400]\tvalid_0's l1: 6.01181\n",
      "[4500]\tvalid_0's l1: 6.00733\n",
      "[4600]\tvalid_0's l1: 6.00332\n",
      "[4700]\tvalid_0's l1: 5.99909\n",
      "[4800]\tvalid_0's l1: 5.99513\n",
      "[4900]\tvalid_0's l1: 5.99057\n",
      "[5000]\tvalid_0's l1: 5.98631\n",
      "[5100]\tvalid_0's l1: 5.98224\n",
      "[5200]\tvalid_0's l1: 5.97813\n",
      "[5300]\tvalid_0's l1: 5.97502\n",
      "[5400]\tvalid_0's l1: 5.97198\n",
      "[5500]\tvalid_0's l1: 5.96857\n",
      "[5600]\tvalid_0's l1: 5.9642\n",
      "[5700]\tvalid_0's l1: 5.95988\n",
      "[5800]\tvalid_0's l1: 5.95575\n",
      "[5900]\tvalid_0's l1: 5.95196\n",
      "[6000]\tvalid_0's l1: 5.94883\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\tvalid_0's l1: 5.94883\n",
      "Model for fold 2 saved to modelitos_para_despues/doblez_2.txt\n",
      "Fold 2 MAE: 5.948834468436852\n",
      "Fold 3 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.32928\n",
      "[200]\tvalid_0's l1: 6.28847\n",
      "[300]\tvalid_0's l1: 6.2609\n",
      "[400]\tvalid_0's l1: 6.23655\n",
      "[500]\tvalid_0's l1: 6.21582\n",
      "[600]\tvalid_0's l1: 6.19624\n",
      "[700]\tvalid_0's l1: 6.1786\n",
      "[800]\tvalid_0's l1: 6.16251\n",
      "[900]\tvalid_0's l1: 6.1475\n",
      "[1000]\tvalid_0's l1: 6.1342\n",
      "[1100]\tvalid_0's l1: 6.12145\n",
      "[1200]\tvalid_0's l1: 6.10963\n",
      "[1300]\tvalid_0's l1: 6.09847\n",
      "[1400]\tvalid_0's l1: 6.08785\n",
      "[1500]\tvalid_0's l1: 6.07693\n",
      "[1600]\tvalid_0's l1: 6.0664\n",
      "[1700]\tvalid_0's l1: 6.05667\n",
      "[1800]\tvalid_0's l1: 6.0478\n",
      "[1900]\tvalid_0's l1: 6.03884\n",
      "[2000]\tvalid_0's l1: 6.02994\n",
      "[2100]\tvalid_0's l1: 6.01989\n",
      "[2200]\tvalid_0's l1: 6.01\n",
      "[2300]\tvalid_0's l1: 6.00133\n",
      "[2400]\tvalid_0's l1: 5.99375\n",
      "[2500]\tvalid_0's l1: 5.98561\n",
      "[2600]\tvalid_0's l1: 5.97856\n",
      "[2700]\tvalid_0's l1: 5.9707\n",
      "[2800]\tvalid_0's l1: 5.96405\n",
      "[2900]\tvalid_0's l1: 5.95657\n",
      "[3000]\tvalid_0's l1: 5.95043\n",
      "[3100]\tvalid_0's l1: 5.9433\n",
      "[3200]\tvalid_0's l1: 5.93656\n",
      "[3300]\tvalid_0's l1: 5.92984\n",
      "[3400]\tvalid_0's l1: 5.92326\n",
      "[3500]\tvalid_0's l1: 5.91706\n",
      "[3600]\tvalid_0's l1: 5.90974\n",
      "[3700]\tvalid_0's l1: 5.90361\n",
      "[3800]\tvalid_0's l1: 5.89739\n",
      "[3900]\tvalid_0's l1: 5.89164\n",
      "[4000]\tvalid_0's l1: 5.88483\n",
      "[4100]\tvalid_0's l1: 5.87831\n",
      "[4200]\tvalid_0's l1: 5.87189\n",
      "[4300]\tvalid_0's l1: 5.86562\n",
      "[4400]\tvalid_0's l1: 5.85957\n",
      "[4500]\tvalid_0's l1: 5.85365\n",
      "[4600]\tvalid_0's l1: 5.8484\n",
      "[4700]\tvalid_0's l1: 5.84317\n",
      "[4800]\tvalid_0's l1: 5.83924\n",
      "[4900]\tvalid_0's l1: 5.8352\n",
      "[5000]\tvalid_0's l1: 5.83132\n",
      "[5100]\tvalid_0's l1: 5.82732\n",
      "[5200]\tvalid_0's l1: 5.8238\n",
      "[5300]\tvalid_0's l1: 5.81874\n",
      "[5400]\tvalid_0's l1: 5.81359\n",
      "[5500]\tvalid_0's l1: 5.8088\n",
      "[5600]\tvalid_0's l1: 5.80412\n",
      "[5700]\tvalid_0's l1: 5.79947\n",
      "[5800]\tvalid_0's l1: 5.79535\n",
      "[5900]\tvalid_0's l1: 5.79006\n",
      "[6000]\tvalid_0's l1: 5.78478\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\tvalid_0's l1: 5.78478\n",
      "Model for fold 3 saved to modelitos_para_despues/doblez_3.txt\n",
      "Fold 3 MAE: 5.784775499951493\n",
      "Fold 4 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.96712\n",
      "[200]\tvalid_0's l1: 5.92233\n",
      "[300]\tvalid_0's l1: 5.88899\n",
      "[400]\tvalid_0's l1: 5.86096\n",
      "[500]\tvalid_0's l1: 5.83529\n",
      "[600]\tvalid_0's l1: 5.81234\n",
      "[700]\tvalid_0's l1: 5.79137\n",
      "[800]\tvalid_0's l1: 5.77127\n",
      "[900]\tvalid_0's l1: 5.75298\n",
      "[1000]\tvalid_0's l1: 5.7352\n",
      "[1100]\tvalid_0's l1: 5.71905\n",
      "[1200]\tvalid_0's l1: 5.70308\n",
      "[1300]\tvalid_0's l1: 5.68897\n",
      "[1400]\tvalid_0's l1: 5.67551\n",
      "[1500]\tvalid_0's l1: 5.66221\n",
      "[1600]\tvalid_0's l1: 5.64939\n",
      "[1700]\tvalid_0's l1: 5.63657\n",
      "[1800]\tvalid_0's l1: 5.62454\n",
      "[1900]\tvalid_0's l1: 5.61262\n",
      "[2000]\tvalid_0's l1: 5.60108\n",
      "[2100]\tvalid_0's l1: 5.59001\n",
      "[2200]\tvalid_0's l1: 5.57973\n",
      "[2300]\tvalid_0's l1: 5.56959\n",
      "[2400]\tvalid_0's l1: 5.56006\n",
      "[2500]\tvalid_0's l1: 5.55122\n",
      "[2600]\tvalid_0's l1: 5.54198\n",
      "[2700]\tvalid_0's l1: 5.53333\n",
      "[2800]\tvalid_0's l1: 5.52464\n",
      "[2900]\tvalid_0's l1: 5.51638\n",
      "[3000]\tvalid_0's l1: 5.50806\n",
      "[3100]\tvalid_0's l1: 5.49908\n",
      "[3200]\tvalid_0's l1: 5.48981\n",
      "[3300]\tvalid_0's l1: 5.48127\n",
      "[3400]\tvalid_0's l1: 5.47267\n",
      "[3500]\tvalid_0's l1: 5.46444\n",
      "[3600]\tvalid_0's l1: 5.45701\n",
      "[3700]\tvalid_0's l1: 5.44935\n",
      "[3800]\tvalid_0's l1: 5.4427\n",
      "[3900]\tvalid_0's l1: 5.43543\n",
      "[4000]\tvalid_0's l1: 5.42964\n",
      "[4100]\tvalid_0's l1: 5.42387\n",
      "[4200]\tvalid_0's l1: 5.41747\n",
      "[4300]\tvalid_0's l1: 5.41168\n",
      "[4400]\tvalid_0's l1: 5.40591\n",
      "[4500]\tvalid_0's l1: 5.4007\n",
      "[4600]\tvalid_0's l1: 5.39588\n",
      "[4700]\tvalid_0's l1: 5.39182\n",
      "[4800]\tvalid_0's l1: 5.3873\n",
      "[4900]\tvalid_0's l1: 5.38252\n",
      "[5000]\tvalid_0's l1: 5.37761\n",
      "[5100]\tvalid_0's l1: 5.37313\n",
      "[5200]\tvalid_0's l1: 5.36836\n",
      "[5300]\tvalid_0's l1: 5.36388\n",
      "[5400]\tvalid_0's l1: 5.35975\n",
      "[5500]\tvalid_0's l1: 5.35549\n",
      "[5600]\tvalid_0's l1: 5.35119\n",
      "[5700]\tvalid_0's l1: 5.34784\n",
      "[5800]\tvalid_0's l1: 5.34422\n",
      "[5900]\tvalid_0's l1: 5.34126\n",
      "[6000]\tvalid_0's l1: 5.33913\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\tvalid_0's l1: 5.33913\n",
      "Model for fold 4 saved to modelitos_para_despues/doblez_4.txt\n",
      "Fold 4 MAE: 5.33912843099943\n",
      "Fold 5 Model Training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 4.83973\n",
      "[200]\tvalid_0's l1: 4.82316\n",
      "[300]\tvalid_0's l1: 4.81304\n",
      "[400]\tvalid_0's l1: 4.80665\n",
      "[500]\tvalid_0's l1: 4.80408\n",
      "[600]\tvalid_0's l1: 4.80193\n",
      "[700]\tvalid_0's l1: 4.8004\n",
      "[800]\tvalid_0's l1: 4.798\n",
      "[900]\tvalid_0's l1: 4.79762\n",
      "[1000]\tvalid_0's l1: 4.7969\n",
      "[1100]\tvalid_0's l1: 4.79623\n",
      "[1200]\tvalid_0's l1: 4.79494\n",
      "[1300]\tvalid_0's l1: 4.79421\n",
      "[1400]\tvalid_0's l1: 4.79409\n",
      "Early stopping, best iteration is:\n",
      "[1352]\tvalid_0's l1: 4.79367\n",
      "Model for fold 5 saved to modelitos_para_despues/doblez_5.txt\n",
      "Fold 5 MAE: 4.793665203427053\n",
      "Training final model with average best iteration: 5070\n",
      "Final model saved to modelitos_para_despues/doblez-conjunto.txt\n",
      "Average MAE across all folds: 5.688230603541532\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"mae\",\n",
    "    \"n_estimators\": 6000,\n",
    "    \"num_leaves\": 384,\n",
    "    \"subsample\": 0.6,  # 0.6和0.8没区别\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"learning_rate\": 0.00871,\n",
    "    'max_depth': 11,\n",
    "    \"n_jobs\": 4,\n",
    "    \"device\": \"gpu\",\n",
    "    \"verbosity\": -1,\n",
    "    \"importance_type\": \"gain\",\n",
    "}\n",
    "feature_name = list(df_train_feats.columns)\n",
    "print(f\"Feature length = {len(feature_name)}\")\n",
    "\n",
    "num_folds = 5\n",
    "fold_size = 480 // num_folds\n",
    "gap = 5\n",
    "\n",
    "models = []\n",
    "scores = []\n",
    "\n",
    "model_save_path = 'modelitos_para_despues' \n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "date_ids = df['date_id'].values\n",
    "\n",
    "for i in range(num_folds):\n",
    "    start = i * fold_size\n",
    "    end = start + fold_size\n",
    "    if i < num_folds - 1:  # No need to purge after the last fold\n",
    "        purged_start = end - 2\n",
    "        purged_end = end + gap + 2\n",
    "        train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "    else:\n",
    "        train_indices = (date_ids >= start) & (date_ids < end)\n",
    "    \n",
    "    test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "    \n",
    "#     df_fold_train = df_train_feats[train_indices]\n",
    "#     df_fold_train_target = df['target'][train_indices]\n",
    "#     df_fold_valid = df_train_feats[test_indices]\n",
    "#     df_fold_valid_target = df['target'][test_indices]\n",
    "\n",
    "    print(f\"Fold {i+1} Model Training\")\n",
    "    \n",
    "    # Train a LightGBM model for the current fold\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_train_feats[train_indices][feature_name],\n",
    "        df['target'][train_indices],\n",
    "        eval_set=[(df_train_feats[test_indices][feature_name], df['target'][test_indices])],\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=100),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    models.append(lgb_model)\n",
    "    # Save the model to a file\n",
    "    model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n",
    "    lgb_model.booster_.save_model(model_filename)\n",
    "    print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "    # Evaluate model performance on the validation set\n",
    "    fold_predictions = lgb_model.predict(df_train_feats[test_indices][feature_name])\n",
    "    fold_score = mean_absolute_error(fold_predictions, df['target'][test_indices])\n",
    "    scores.append(fold_score)\n",
    "    print(f\"Fold {i+1} MAE: {fold_score}\")\n",
    "\n",
    "    # Free up memory by deleting fold specific variables\n",
    "#     del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "    gc.collect()\n",
    "\n",
    "# Calculate the average best iteration from all regular folds\n",
    "average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "# Update the lgb_params with the average best iteration\n",
    "final_model_params = lgb_params.copy()\n",
    "final_model_params['n_estimators'] = average_best_iteration\n",
    "\n",
    "print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "final_model.fit(\n",
    "    df_train_feats[feature_name],\n",
    "    df['target'],\n",
    "    callbacks=[\n",
    "        lgb.callback.log_evaluation(period=100),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Append the final model to the list of models\n",
    "models.append(final_model)\n",
    "\n",
    "# Save the final model to a file\n",
    "final_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\n",
    "final_model.booster_.save_model(final_model_filename)\n",
    "print(f\"Final model saved to {final_model_filename}\")\n",
    "\n",
    "# Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\n",
    "print(f\"Average MAE across all folds: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa102f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T06:27:58.663746Z",
     "iopub.status.busy": "2023-11-09T06:27:58.662474Z",
     "iopub.status.idle": "2023-11-09T06:27:58.668816Z",
     "shell.execute_reply": "2023-11-09T06:27:58.667893Z"
    },
    "papermill": {
     "duration": 0.046052,
     "end_time": "2023-11-09T06:27:58.671872",
     "exception": false,
     "start_time": "2023-11-09T06:27:58.625820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Testing\n",
    "# def zero_sum(prices, volumes):\n",
    "#     std_error = np.sqrt(volumes)\n",
    "#     step = np.sum(prices)/np.sum(std_error)\n",
    "#     out = prices-std_error*step\n",
    "#     return out\n",
    "\n",
    "# if is_infer:\n",
    "#     import optiver2023\n",
    "#     env = optiver2023.make_env()\n",
    "#     iter_test = env.iter_test()\n",
    "#     counter = 0\n",
    "#     y_min, y_max = -64, 64\n",
    "#     qps, predictions = [], []\n",
    "#     cache = pd.DataFrame()\n",
    "#     for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "#         now_time = time.time()\n",
    "#         cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "#         if counter > 0:\n",
    "#             cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "#         feat = generate_all_features(cache)[-len(test):]\n",
    "#         lgb_prediction = infer_lgb_model.predict(feat)\n",
    "# #         lgb_prediction = zero_sum(lgb_prediction, test['bid_size'] + test['ask_size'])\n",
    "#         lgb_prediction = lgb_prediction - np.mean(lgb_prediction)\n",
    "#         clipped_predictions = np.clip(lgb_prediction, y_min, y_max)\n",
    "#         sample_prediction['target'] = clipped_predictions\n",
    "#         env.predict(sample_prediction)\n",
    "#         counter += 1\n",
    "#         qps.append(time.time() - now_time)\n",
    "#         if counter % 10 == 0:\n",
    "#             print(counter, 'qps:', np.mean(qps))\n",
    "#     time_cost = 1.146 * np.mean(qps)\n",
    "#     print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7055493a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-09T06:27:58.729981Z",
     "iopub.status.busy": "2023-11-09T06:27:58.729634Z",
     "iopub.status.idle": "2023-11-09T06:33:00.403056Z",
     "shell.execute_reply": "2023-11-09T06:33:00.401980Z"
    },
    "papermill": {
     "duration": 301.702546,
     "end_time": "2023-11-09T06:33:00.405175",
     "exception": false,
     "start_time": "2023-11-09T06:27:58.702629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "10 qps: 1.46881103515625\n",
      "20 qps: 1.5808643341064452\n",
      "30 qps: 1.6635185480117798\n",
      "40 qps: 1.7057941436767579\n",
      "50 qps: 1.736162166595459\n",
      "60 qps: 1.7592510024706522\n",
      "70 qps: 1.770152143069676\n",
      "80 qps: 1.7811688750982284\n",
      "90 qps: 1.7888750500149198\n",
      "100 qps: 1.7978293251991273\n",
      "110 qps: 1.8059961232272062\n",
      "120 qps: 1.809061481555303\n",
      "130 qps: 1.8133647221785325\n",
      "140 qps: 1.8142146706581115\n",
      "150 qps: 1.8178698174158732\n",
      "160 qps: 1.8203248128294944\n",
      "The code will take approximately 2.089 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    # Weights for each fold model\n",
    "    model_weights = [1/len(models)] * len(models) \n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "\n",
    "        # Generate predictions for each model and calculate the weighted average\n",
    "        lgb_predictions = np.zeros(len(test))\n",
    "        for model, weight in zip(models, model_weights):\n",
    "            lgb_predictions += weight * model.predict(feat)\n",
    "\n",
    "        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n",
    "        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13382.830349,
   "end_time": "2023-11-09T06:33:01.982463",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-09T02:49:59.152114",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}