{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7dad77",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-29T21:32:51.898919Z",
     "iopub.status.busy": "2023-10-29T21:32:51.898115Z",
     "iopub.status.idle": "2023-10-29T21:32:58.485535Z",
     "shell.execute_reply": "2023-10-29T21:32:58.484083Z"
    },
    "papermill": {
     "duration": 6.59707,
     "end_time": "2023-10-29T21:32:58.488470",
     "exception": false,
     "start_time": "2023-10-29T21:32:51.891400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import tqdm\n",
    "from warnings import simplefilter\n",
    "from typing import List\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8156fdc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T21:32:58.504583Z",
     "iopub.status.busy": "2023-10-29T21:32:58.503194Z",
     "iopub.status.idle": "2023-10-29T21:33:18.507995Z",
     "shell.execute_reply": "2023-10-29T21:33:18.506793Z"
    },
    "papermill": {
     "duration": 20.014583,
     "end_time": "2023-10-29T21:33:18.510630",
     "exception": false,
     "start_time": "2023-10-29T21:32:58.496047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270fd0bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T21:33:18.521671Z",
     "iopub.status.busy": "2023-10-29T21:33:18.521259Z",
     "iopub.status.idle": "2023-10-29T21:35:08.242330Z",
     "shell.execute_reply": "2023-10-29T21:35:08.241095Z"
    },
    "papermill": {
     "duration": 109.730143,
     "end_time": "2023-10-29T21:35:08.245402",
     "exception": false,
     "start_time": "2023-10-29T21:33:18.515259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#整体特征\n",
    "median_sizes = train.groupby('stock_id')['bid_size'].median() + train.groupby('stock_id')['ask_size'].median()\n",
    "std_sizes = train.groupby('stock_id')['bid_size'].std() + train.groupby('stock_id')['ask_size'].std()\n",
    "max_sizes = train.groupby('stock_id')['bid_size'].max() + train.groupby('stock_id')['ask_size'].max()\n",
    "min_sizes = train.groupby('stock_id')['bid_size'].min() + train.groupby('stock_id')['ask_size'].min()\n",
    "mean_sizes = train.groupby('stock_id')['bid_size'].mean() + train.groupby('stock_id')['ask_size'].mean()\n",
    "first_sizes = train.groupby('stock_id')['bid_size'].first() + train.groupby('stock_id')['ask_size'].first()\n",
    "last_sizes = train.groupby('stock_id')['bid_size'].last() + train.groupby('stock_id')['ask_size'].last()\n",
    "#可以再做日期的（好像没看到drop掉日期列）\n",
    "\n",
    "train = train.dropna(subset=['target'])\n",
    "\n",
    "def feature_eng(df):\n",
    "    cols = [c for c in df.columns if c not in ['row_id', 'date_id','time_id']]\n",
    "    df = df[cols]\n",
    "    \n",
    "    #匹配失败数量和匹配成功数量的比率\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / df['matched_size']\n",
    "    #供需市场的差额\n",
    "    df['bid_ask_volume_diff'] = df['ask_size'] - df['bid_size']\n",
    "    #供需市场总和\n",
    "    df['bid_plus_ask_sizes'] = df['bid_size'] + df['ask_size']\n",
    "    \n",
    "    #供需价格的均值\n",
    "    df['mid_price'] = (df['ask_price'] + df['bid_price']) / 2\n",
    "    \n",
    "    #整体数据情况\n",
    "    df['median_size'] = df['stock_id'].map(median_sizes.to_dict())\n",
    "    df['std_size'] = df['stock_id'].map(std_sizes.to_dict())\n",
    "    df['max_size'] = df['stock_id'].map(max_sizes.to_dict())\n",
    "    df['min_size'] = df['stock_id'].map(min_sizes.to_dict())\n",
    "    df['mean_size'] = df['stock_id'].map(mean_sizes.to_dict())\n",
    "    df['first_size'] = df['stock_id'].map(first_sizes.to_dict())    \n",
    "    df['last_size'] = df['stock_id'].map(last_sizes.to_dict())       \n",
    "    \n",
    "    #整体市场规模和当前的市场规模比较\n",
    "    df['high_volume'] = np.where(df['bid_plus_ask_sizes'] > df['median_size'], 1, 0)\n",
    "    \n",
    "    prices = ['reference_price', 'far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    \n",
    "    #价格之间做差，做差/求和\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f'{c[0]}_minus_{c[1]}'] = (df[f'{c[0]}'] - df[f'{c[1]}']).astype(np.float32)\n",
    "        df[f'{c[0]}_{c[1]}_imb'] = df.eval(f'({c[0]} - {c[1]})/({c[0]} + {c[1]})')\n",
    "        \n",
    "    for c in combinations(prices, 3):\n",
    "        max_ = df[list(c)].max(axis=1)\n",
    "        min_ = df[list(c)].min(axis=1)\n",
    "        mid_ = df[list(c)].sum(axis=1) - min_ - max_\n",
    "        \n",
    "        df[f'{c[0]}_{c[1]}_{c[2]}_imb2'] = (max_-mid_)/(mid_-min_ + 1e-4)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "y = train['target'].values\n",
    "X = feature_eng(train.drop(columns='target'))\n",
    "\n",
    "y_min = np.min(y)\n",
    "y_max = np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b536e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T21:35:08.256379Z",
     "iopub.status.busy": "2023-10-29T21:35:08.255973Z",
     "iopub.status.idle": "2023-10-29T21:35:12.827038Z",
     "shell.execute_reply": "2023-10-29T21:35:12.825655Z"
    },
    "papermill": {
     "duration": 4.580072,
     "end_time": "2023-10-29T21:35:12.830206",
     "exception": false,
     "start_time": "2023-10-29T21:35:08.250134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imbalance_size                         float64\n",
      "reference_price                        float64\n",
      "matched_size                           float64\n",
      "far_price                              float64\n",
      "near_price                             float64\n",
      "                                        ...   \n",
      "far_price_bid_price_wap_imb2           float64\n",
      "near_price_ask_price_bid_price_imb2    float64\n",
      "near_price_ask_price_wap_imb2          float64\n",
      "near_price_bid_price_wap_imb2          float64\n",
      "ask_price_bid_price_wap_imb2           float64\n",
      "Length: 71, dtype: object\n",
      "\n",
      "stock_id                   int64\n",
      "imbalance_buy_sell_flag    int64\n",
      "high_volume                int64\n",
      "stage                      int64\n",
      "min_in_bucket              int64\n",
      "dtype: object\n",
      "   stock_id  imbalance_buy_sell_flag  high_volume  stage  min_in_bucket\n",
      "0         0                        2            1      0              0\n",
      "1         1                        0            0      0              0\n",
      "2         2                        0            1      0              0\n",
      "3         3                        0            1      0              0\n",
      "4         4                        0            0      0              0\n",
      "stock_id                   0\n",
      "imbalance_buy_sell_flag    0\n",
      "high_volume                0\n",
      "stage                      0\n",
      "min_in_bucket              0\n",
      "dtype: int64\n",
      "stock_id                   200\n",
      "imbalance_buy_sell_flag      3\n",
      "high_volume                  2\n",
      "stage                        2\n",
      "min_in_bucket                9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def feat_eng_nn(df: pd.DataFrame):\n",
    "    # change seconds_in_bucket to 9 categories (9 min) & make a new col\n",
    "    df[\"stage\"] = np.where(df[\"seconds_in_bucket\"] > 300, 1, 0)\n",
    "    df[\"min_in_bucket\"] = df[\"seconds_in_bucket\"]\n",
    "    for i in range(9):\n",
    "        t1, t2 = i * 60, ((i+1) * 60 if i < 8 else 541 )\n",
    "        df.loc[(df[\"min_in_bucket\"] >= t1) & (df[\"min_in_bucket\"] < t2), \"min_in_bucket\"] = i \n",
    "\n",
    "    # create discrete feature\n",
    "    int_feat = df.dtypes[df.dtypes == \"int64\"].to_dict().keys()\n",
    "    \n",
    "    # handle invaild values\n",
    "    X_dsc = df[int_feat]\n",
    "    for f in int_feat:\n",
    "        mv = np.min(X_dsc[f])\n",
    "        if mv < 0:\n",
    "            X_dsc[f] += 0 - mv\n",
    "    X_dsc = X_dsc.drop(columns=\"seconds_in_bucket\")\n",
    "    assert not X_dsc.isnull().any().any()\n",
    "    cat_num = X_dsc.nunique()\n",
    "    \n",
    "    X_ctg = df.drop(columns=int_feat)\n",
    "    X_ctg = X_ctg.fillna(0)\n",
    "    \n",
    "    \n",
    "    return X_ctg, X_dsc, cat_num\n",
    "\n",
    "X_ctg, X_dsc, cat_num = feat_eng_nn(X)\n",
    "print(X_ctg.dtypes)\n",
    "print()\n",
    "print(X_dsc.dtypes)\n",
    "print(X_dsc.head())\n",
    "print(X_dsc.min())\n",
    "print(cat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4779f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T21:35:12.841725Z",
     "iopub.status.busy": "2023-10-29T21:35:12.841322Z",
     "iopub.status.idle": "2023-10-29T21:35:14.183594Z",
     "shell.execute_reply": "2023-10-29T21:35:14.182275Z"
    },
    "papermill": {
     "duration": 1.351043,
     "end_time": "2023-10-29T21:35:14.186118",
     "exception": false,
     "start_time": "2023-10-29T21:35:12.835075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "imbalance_size                         2.982028e+09\n",
      "reference_price                        1.077488e+00\n",
      "matched_size                           7.713682e+09\n",
      "far_price                              4.379531e+02\n",
      "near_price                             1.309732e+00\n",
      "                                           ...     \n",
      "far_price_bid_price_wap_imb2           2.229371e+06\n",
      "near_price_ask_price_bid_price_imb2    6.537748e+02\n",
      "near_price_ask_price_wap_imb2          2.478137e+03\n",
      "near_price_bid_price_wap_imb2          1.004060e+03\n",
      "ask_price_bid_price_wap_imb2           1.090000e+02\n",
      "Length: 71, dtype: float64\n",
      "2229370.8366136355\n"
     ]
    }
   ],
   "source": [
    "print(np.isinf(X_ctg).any().any())\n",
    "print(X_ctg.max())\n",
    "# X_ctg[\"far_price_bid_price_wap_imb2\"].replace(np.inf, 0)\n",
    "print(np.max(X_ctg[\"far_price_bid_price_wap_imb2\"].replace(np.inf, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa52daef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T21:35:14.198167Z",
     "iopub.status.busy": "2023-10-29T21:35:14.196998Z",
     "iopub.status.idle": "2023-10-29T21:35:14.207113Z",
     "shell.execute_reply": "2023-10-29T21:35:14.206105Z"
    },
    "papermill": {
     "duration": 0.018561,
     "end_time": "2023-10-29T21:35:14.209614",
     "exception": false,
     "start_time": "2023-10-29T21:35:14.191053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NNDataset(Dataset):\n",
    "    def __init__(self, X_c, X_d, y_nn):\n",
    "        self.X_c = X_c\n",
    "        self.X_d = X_d\n",
    "        self.y_nn = y_nn\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_d)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y_nn is None:\n",
    "            return torch.tensor(self.X_c.iloc[idx]).float(), torch.tensor(self.X_d.iloc[idx]).long()\n",
    "        return torch.tensor(self.X_c.iloc[idx]).float(), torch.tensor(self.X_d.iloc[idx]).long(), torch.tensor(self.y_nn[idx]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041a85b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T21:35:14.222031Z",
     "iopub.status.busy": "2023-10-29T21:35:14.220942Z",
     "iopub.status.idle": "2023-10-29T21:35:14.248134Z",
     "shell.execute_reply": "2023-10-29T21:35:14.246922Z"
    },
    "papermill": {
     "duration": 0.036551,
     "end_time": "2023-10-29T21:35:14.251239",
     "exception": false,
     "start_time": "2023-10-29T21:35:14.214688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07d9933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T21:35:14.263362Z",
     "iopub.status.busy": "2023-10-29T21:35:14.262612Z",
     "iopub.status.idle": "2023-10-29T21:35:14.293958Z",
     "shell.execute_reply": "2023-10-29T21:35:14.292471Z"
    },
    "papermill": {
     "duration": 0.040232,
     "end_time": "2023-10-29T21:35:14.296346",
     "exception": false,
     "start_time": "2023-10-29T21:35:14.256114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5237892 71\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "N_Folds = 4\n",
    "kf = KFold(n_splits=N_Folds, shuffle=True, random_state=2023)\n",
    "is_train = True\n",
    "params_nn = {\n",
    "    \"batch_size\": 1280,\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 35,\n",
    "    \"val_iter\": 1500,\n",
    "    \"train_log_step\": 500,\n",
    "    \"scheduler_factor\":0.25,\n",
    "    \"scd_patience\": 2,\n",
    "    \"patience\": 3\n",
    "}\n",
    "\n",
    "N, D_c = X_ctg.shape\n",
    "print(N, D_c)\n",
    "embed_dims = list(cat_num.to_dict().values())\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "output_dir = \"cnn_models\"\n",
    "os.system(f'mkdir {output_dir}')\n",
    "if is_train:\n",
    "    for fold_i, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
    "        # data\n",
    "        tr_X_ctg, tr_X_dsc, tr_y = X_ctg.iloc[train_idx], X_dsc.iloc[train_idx], y[train_idx]\n",
    "        valid_idx_small = valid_idx[:600000]\n",
    "        val_X_ctg, val_X_dsc, val_y = X_ctg.iloc[valid_idx_small], X_dsc.iloc[valid_idx_small], y[valid_idx_small]\n",
    "        \n",
    "        # build torch dataset and dataloader \n",
    "        dataset_tr = NNDataset(tr_X_ctg, tr_X_dsc, tr_y)\n",
    "        dataset_val = NNDataset(val_X_ctg, val_X_dsc, val_y)\n",
    "        \n",
    "        loader_tr = DataLoader(dataset_tr, batch_size=params_nn[\"batch_size\"], shuffle=True, num_workers=16)\n",
    "        loader_val = DataLoader(dataset_val, batch_size=params_nn[\"batch_size\"], shuffle=False, num_workers=16)\n",
    "        \n",
    "        # build model and related modules\n",
    "        model = CNN(num_features=D_c,\n",
    "                    n_categories=embed_dims,\n",
    "                    hidden_size=8*128,\n",
    "                    emb_dim=16,\n",
    "                    dropout_cat=0,\n",
    "                    channel_1=128,\n",
    "                    channel_2=3*128,\n",
    "                    channel_3=3*128,\n",
    "                    dropout_top=0,\n",
    "                    dropout_mid=0,\n",
    "                    dropout_bottom=0,\n",
    "                    weight_norm=True,\n",
    "                    two_stage=False,\n",
    "                    celu=False,\n",
    "                    kernel1=5,\n",
    "                    leaky_relu=False)\n",
    "        model.to(device)\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params_nn[\"lr\"])\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", \n",
    "                                                         factor=params_nn[\"scheduler_factor\"],\n",
    "                                                         patience=params_nn[\"scd_patience\"])\n",
    "        \n",
    "        # begin training\n",
    "        n_iter = np.ceil(len(dataset_tr) / params_nn[\"batch_size\"])\n",
    "        best_loss = np.inf\n",
    "        last_epoch_loss = np.inf\n",
    "        p = 0\n",
    "        for epoch in range(params_nn[\"epochs\"]):\n",
    "            loss_epoch = []\n",
    "            for i, (x_c, x_d, y_tr) in enumerate(loader_tr):\n",
    "                optimizer.zero_grad()\n",
    "                x_c, x_d, y_tr = x_c.to(device), x_d.to(device), y_tr.to(device)\n",
    "                y_hat = model(x_c, x_d)\n",
    "                loss = criterion(y_hat, y_tr)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_epoch.append(loss.detach().item())\n",
    "                \n",
    "                # log\n",
    "                if i != 0 and i % params_nn[\"train_log_step\"] == 0:\n",
    "                    print(f\"Fold {fold_i:1}, \"\n",
    "                          f\"Epoch {epoch:2}/{params_nn['epochs']:2}, \"\n",
    "                          f\"iter {i:5}/{n_iter:5}, \"\n",
    "                          f\"loss {np.mean(loss_epoch[-params_nn['train_log_step']:]):8.4f}\")\n",
    "                \n",
    "                # validate\n",
    "                if i % params_nn[\"val_iter\"] == 0 and i != 0 or i == n_iter - 1:\n",
    "                    loss_val = []\n",
    "#                     bar = tqdm.tqdm(total=np.ceil(len(dataset_val) / params_nn[\"batch_size\"]))\n",
    "                    with torch.no_grad():\n",
    "                        for i, (x_c, x_d, y_val) in enumerate(loader_val):\n",
    "                            x_c, x_d, y_val = x_c.to(device), x_d.to(device), y_val.to(device)\n",
    "\n",
    "                            y_hat = model(x_c, x_d)\n",
    "                            loss = criterion(y_hat, y_val)\n",
    "\n",
    "                            loss_val.append(loss.item())\n",
    "                            bar.update()\n",
    "                    \n",
    "                    loss_val_avg = np.mean(loss_val)\n",
    "                    print(f\"==> Val current best loss {best_loss:8.4f}\")\n",
    "                    print(f\"Fold {fold_i:1}, \"\n",
    "                          f\"Epoch {epoch:2}/{params_nn['epochs']:2}, \"\n",
    "                          f\"Valid loss {loss_val_avg:8.4f}\")\n",
    "                    if loss_val_avg < best_loss:\n",
    "                        best_loss = loss_val_avg\n",
    "                        print(f\"Loss decreases! current best loss {best_loss:8.4f}\")\n",
    "                        torch.save(model, os.path.join(output_dir, f\"fold{fold_i}.pt\"))\n",
    "                    print()\n",
    "            \n",
    "            # early stop\n",
    "            if loss_val_avg >= last_epoch_loss:\n",
    "                p += 1\n",
    "                print(f\"Best loss doesn't decrease in this epoch, patience {p}/{params_nn['patience']}\")\n",
    "                if p >= params_nn[\"patience\"]:\n",
    "                    print(f\"Reach patience, quit training of fold {fold_i}\")\n",
    "                    print()\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Epoch loss decreases from {last_epoch_loss:8.4f} to {loss_val_avg:8.4f}\")\n",
    "                last_epoch_loss = loss_val_avg\n",
    "                p = 0\n",
    "            \n",
    "            # scheduler\n",
    "            scheduler.step(loss_val_avg)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ae9f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T21:35:14.308201Z",
     "iopub.status.busy": "2023-10-29T21:35:14.307766Z",
     "iopub.status.idle": "2023-10-29T21:38:25.265556Z",
     "shell.execute_reply": "2023-10-29T21:38:25.264288Z"
    },
    "papermill": {
     "duration": 190.967799,
     "end_time": "2023-10-29T21:38:25.269247",
     "exception": false,
     "start_time": "2023-10-29T21:35:14.301448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "    \n",
    "    return out\n",
    "\n",
    "is_infer = False\n",
    "N_Folds = 4\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    predictions = []\n",
    "    \n",
    "    batch_size_test = 128\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        feat = feature_eng(test)\n",
    "        X_ctg, X_dsc, cat_num = feat_eng_nn(feat)\n",
    "        test_dataset = NNDataset(X_ctg, X_dsc, None)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=batch_size_test,shuffle=False)\n",
    "        \n",
    "        fold_prediction = np.zeros((test.shape[0],))\n",
    "        for fold in range(0, N_Folds):\n",
    "            model_filename = f\"/kaggle/input/cnn-optiver/cnn_models/fold{fold}.pt\"\n",
    "            m = torch.load(model_filename, map_location=\"cpu\")\n",
    "            for data_i, (x_c, x_d) in enumerate(test_loader):\n",
    "                y_hat = m(x_c, x_d)\n",
    "                fold_prediction[data_i * batch_size_test: (data_i + 1) * batch_size_test] += y_hat.detach().cpu().numpy()\n",
    "                \n",
    "        fold_prediction /= N_Folds\n",
    "        fold_prediction = zero_sum(fold_prediction, test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "        clipped_predictions = np.clip(fold_prediction, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 338.412621,
   "end_time": "2023-10-29T21:38:26.700349",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-29T21:32:48.287728",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}